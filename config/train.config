####################################################
################## Train parameters ################
####################################################
[train_args]
network = grid_6
experiment_time = 3000000
experiment_save_agent = True
experiment_save_agent_interval = 5000
# For multiple runs, seeds are set using run.config
experiment_seed = None
sumo_render = False
sumo_emission = False
tls_type = rl
demand_type = variable



####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
discount_factor = 0.98
action_space = 'discrete'

#################### State space ###################
# features = ('speed', 'count')
# features = ('delay',)
# features = ('delay', 'lag[delay]')
# features = ('queue', 'lag[queue]')
features = ('pressure',)
# features = ('speed_score', 'count')

category_counts = [5, 10, 15, 20, 25, 30]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
category_delays = [5, 10, 25, 50, 75, 100]
category_queues = [1, 3, 10, 15, 20, 25]
category_times = [5, 10, 13, 17, 19, 21]
category_pressures = [-5, -3, 0, 3, 5, 10]
category_speed_scores = [3, 5, 7, 9, 11, 13]
normalize_state_space = True
discretize_state_space = False
time_period = None

###################### Reward ######################
# reward = 'reward_max_speed_count'
# reward = 'reward_min_delay'
# reward = 'reward_max_delay_reduction'
# reward = 'reward_min_queue_squared'
# reward = 'reward_min_pressure'
reward = 'reward_max_speed_score'

reward_rescale = 0.01
target_velocity = 1.0
velocity_threshold = 0.1



####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN

#################### Q-learning ####################
[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 0

######################## DQN #######################
[dqn_args]
learning_rate = 1e-3
n_step = 5
batch_size = 128
target_update_period = 100
min_replay_size = 5000
max_replay_size = 50000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
samples_per_insert = 128.0
prefetch_size = 1

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
# (state -> torso net -> head net -> Q-values).
torso_layers = [5]
head_layers = [5]

####################### R2D2 #######################
[r2d2_args]
learning_rate = 1e-3
burn_in_length = 5
trace_length = 15
replay_period = 5
batch_size = 100
target_update_period = 100
min_replay_size = 1000
max_replay_size = 20000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
max_priority_weight = 0.9
samples_per_insert = 100.0
prefetch_size = 1
store_lstm_state = True

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
rnn_hidden_size = 10
head_layers = [5]

####################### DDPG #######################
[ddpg_args]
batch_size = 128
prefetch_size = 1
target_update_period = 100
min_replay_size = 1000
max_replay_size = 30000
samples_per_insert = 128.0
n_step = 5
clipping = True

# Exploration parameters.
sigma_init = 0.4
sigma_final = 0.01
sigma_schedule_timesteps = 45000

# Neural networks parameters.
policy_layers = [5, 5]
critic_layers = [5, 5]
