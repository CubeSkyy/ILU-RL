####################################################
################## Train parameters ################
####################################################
[train_args]
network = grid
experiment_time = 3000000
experiment_save_agent = True
experiment_save_agent_interval = 5000
# For multiple runs, seeds are set using run.config
experiment_seed = None
sumo_render = False
sumo_emission = False
tls_type = rl
demand_type = variable



####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
discount_factor = 0.98
action_space = 'discrete'

#################### State space ###################
# features = ('speed', 'count')
# features = ('speed_score', 'count')
# features = ('delay',)
# features = ('delay', 'lag[delay]')
features = ('waiting_time',)
# features = ('queue', 'lag[queue]')
# features = ('flow',)
# features = ('pressure',)
# features = ('average_pressure',)

category_average_pressures = [-1.08, -0.37, 0.00, 0.48, 1.02]
category_counts = [0.12, 0.45, 0.88, 1.60, 2.32]
category_delays = [0.0, 0.06, 0.4, 0.82, 1.31]
category_waiting_times = [0.0, 0.03, 0.77, 1.23]
category_flows = [1.0, 2.0, 3.0, 5.0, 6.0]
category_queues = [0.0, 0.03, 0.35, 0.63, 0.97]
category_times = [6, 10, 17, 19, 21]
category_pressures = [-5, -3, 0, 3, 5, 10]
category_speed_scores = [0.14, 0.30, 0.48, 0.77, 0.92]
category_speeds = [0.04, 0.32, 0.56, 0.72, 0.81]

normalize_velocities = True
normalize_vehicles = False
discretize_state_space = False
time_period = None

###################### Reward ######################
# reward = 'reward_max_speed_count'
# reward = 'reward_max_speed_score'
# reward = 'reward_min_delay'
# reward = 'reward_max_delay_reduction'
reward = 'reward_min_waiting_time'
# reward = 'reward_min_queue_squared'
# reward = 'reward_max_flow'
# reward = 'reward_min_pressure'
# reward = 'reward_min_average_pressure'
 

reward_rescale = 0.01
velocity_threshold = 0.1



####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN

#################### Q-learning ####################
[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 0

######################## DQN #######################
[dqn_args]
learning_rate = 1e-3
n_step = 5
batch_size = 128
target_update_period = 100
min_replay_size = 5000
max_replay_size = 50000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
samples_per_insert = 128.0
prefetch_size = 1

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
# (state -> torso net -> head net -> Q-values).
torso_layers = [10]
head_layers = [10]

####################### R2D2 #######################
[r2d2_args]
learning_rate = 1e-3
burn_in_length = 5
trace_length = 15
replay_period = 5
batch_size = 100
target_update_period = 100
min_replay_size = 1000
max_replay_size = 20000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
max_priority_weight = 0.9
samples_per_insert = 100.0
prefetch_size = 1
store_lstm_state = True

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
rnn_hidden_size = 10
head_layers = [10]

####################### DDPG #######################
[ddpg_args]
batch_size = 128
prefetch_size = 1
target_update_period = 100
min_replay_size = 1000
max_replay_size = 30000
samples_per_insert = 128.0
n_step = 5
clipping = True

# Exploration parameters.
sigma_init = 0.4
sigma_final = 0.01
sigma_schedule_timesteps = 45000

# Neural networks parameters.
policy_layers = [10, 10]
critic_layers = [10, 10]
