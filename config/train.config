####################################################
################## Train arguments #################
####################################################
[train_args]
network = grid
experiment_time = 2800
experiment_log = True
experiment_log_interval = 5
experiment_save_agent = True
experiment_save_agent_interval = 10
# For multiple runs, seeds are set using run.config
experiment_seed = None
sumo_render = False
sumo_emission = False
tls_type = controlled
demand_type = constant


####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
states = ('speed', 'count')
category_counts = [5, 10, 15, 20, 25, 30]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
category_delays = [5, 10, 15, 20, 25, 30]
normalize_state_space = True
discretize_state_space = False
target_velocity = 1.0
velocity_threshold = 0.1
reward = 'reward_max_speed_count'
# reward = {'type': 'target_velocity', 'additional_params': {'target_velocity': 1.0}}


####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN


[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
gamma = 0.9
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 200


[dqn_args]
lr = 5e-4
gamma = 0.9
exp_initial_p = 1.0
exp_final_p = 0.01
exp_schedule_timesteps = 45000
learning_starts = 2000
target_net_update_interval = 2000

# Replay buffer parameters.
buffer_size = 30000
batch_size = 200
prioritized_replay = False
prioritized_replay_beta_iters = 50000
prioritized_replay_alpha = 0.6
prioritized_replay_beta0 = 0.4
prioritized_replay_eps = 1e-6

# Q-network parameters.
network_type = mlp
head_network_mlp_hiddens = []
head_network_layer_norm = False
head_network_dueling = False
mlp_hiddens = [8]
mlp_layer_norm = False
