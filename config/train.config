####################################################
################## Train arguments #################
####################################################
[train_args]
network = grid
experiment_time = 1800000
experiment_log = True
experiment_log_interval = 1000
experiment_save_agent = True
experiment_save_agent_interval = 2500
# For multiple runs, seeds are set using run.config
experiment_seed: None
sumo_render = False
sumo_emission = False
tls_type = controlled
inflows_switch = False


####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
states = ('speed', 'count')
category_counts = [5, 10, 15, 20, 25, 30]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
normalize_state_space = True
discretize_state_space = False
reward = {'type': 'target_velocity', 'additional_params': {'target_velocity': 1.0}}


####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN


[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
gamma = 0.9
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 200


[dqn_args]
# TODO: model
lr = 5e-4
gamma = 0.9
buffer_size = 20000
batch_size = 200
exp_initial_p = 1.0
exp_final_p = 0.02
exp_schedule_timesteps = 18000
learning_starts = 1000
target_net_update_interval = 2000