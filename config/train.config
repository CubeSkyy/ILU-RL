[train_args]
network = grid
experiment_time = 18000
experiment_log = True
experiment_log_interval = 100
experiment_save_agent = True
experiment_save_agent_interval = 100
experiment_seed = 20
sumo_render = False
sumo_emission = False
tls_type = controlled
demand_type = constant

[mdp_args]
# states = ('speed', 'count')
# states = ('delay',)
states = ('queue',)
category_counts = [5, 10, 15, 20, 25, 30]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
category_delays = [5, 10, 25, 50, 75, 100]
category_queues = [1, 3, 10, 15, 20, 25]
normalize_state_space = True
discretize_state_space = True
target_velocity = 1.0
velocity_threshold = 0.1
# reward = 'reward_max_speed_count'
# reward = 'reward_min_delay'
reward = 'reward_min_queue_squared'

[agent_type]
agent_type = QL

[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
gamma = 0.9
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 200

[dqn_args]
lr = 5e-4
gamma = 0.9
buffer_size = 30000
batch_size = 200
exp_initial_p = 1.0
exp_final_p = 0.01
exp_schedule_timesteps = 45000
learning_starts = 2000
target_net_update_interval = 2000

