####################################################
################## Train arguments #################
####################################################
[train_args]
network = grid
experiment_time = 4500000
experiment_save_agent = True
experiment_save_agent_interval = 5000
# For multiple runs, seeds are set using run.config
experiment_seed = None
sumo_render = False
sumo_emission = False
tls_type = controlled
demand_type = variable


####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
states = ('speed', 'count')
# states = ('delay',)
# states = ('queue',)
category_counts = [5, 10, 15, 20, 25, 30]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
category_delays = [5, 10, 25, 50, 75, 100]
category_queues = [1, 3, 10, 15, 20, 25]
normalize_state_space = True
discretize_state_space = False
target_velocity = 1.0
velocity_threshold = 0.1
reward = 'reward_max_speed_count'
# reward = 'reward_min_delay'
# reward = 'reward_min_queue_squared'
reward_rescale = 0.01

####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN 


[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
gamma = 0.9
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 0


[dqn_args]
learning_rate = 1e-3
discount = 0.90
batch_size = 128
prefetch_size = 1
target_update_period = 100
samples_per_insert = 64.0
min_replay_size = 1000
max_replay_size = 30000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
n_step = 5
epsilon = 0.05

# TODO: Add network type
# TODO: Add network parameters
# TODO: Add eps-greedy decay
# exp_initial_p = 1.0
# exp_final_p = 0.01
# exp_schedule_timesteps = 45000
