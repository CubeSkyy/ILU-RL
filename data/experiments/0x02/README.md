# Experiment 0x02


The goal is to show convergency wrt Q-tables in intersection scenario.

# MDP
	
- states: (s0, c0, s1, c1)
- actions: (0, 1)
- reward: -max((s\*, s\*) - (s0, s1), 0) \* (c0, c1)
- P: null
- gamma: 0.99

 Where 
 
- sP -> [0, 22) average speed on phase P P=1,2
- cP -> [0, 18] average count on phase P P=1,2 
- s\* target velocity

# Experiment

Experiments are carried out by running the following incantations

```
> time python models/train.py intersection -t 450000 \
			-s 1 -i 1 -r 0 -e 0 -p 1 -W 0 -S 30 -L 60
```

## Settings

- Network: intersection.
- InFlows: `lane`.
- Edges Distribution: ['-238059328', '309265401']
- Phase Splits: 60x30 sec.
- Instances: 6.
- Time Steps: 450000 sec.
- Reward: `target velocity`.
	 
## Observation
 
- Departures only from phase 1.
- Fixed departure times.
- Fixed departure velocities.

## Learning
	
- Algorithm: Q-Learning.
- Upper confidence interval.
- c = 10.
- alpha = 5%.

## Discretization

A discretization scheme is needed by the choice of state value function estimation.

- sP <= 2.28 		-> 0
- 2.28 < sP < 3.11	-> 1
- sP >= 3.11		-> 2
- cP <= 2.28 		-> 0
- 2.28 < cP < 3.11	-> 1
- cP >= 3.11		-> 2

# Results

The states which are visited are:
```
(0, 0, 2, 1), (0, 0, 2, 0), (0, 0, 1, 1)
```

## Distance

We measure two distances one in wrt Q-tables and another to the generated deterministic policies. The results can be generated by issuing:

```
> python analysis/q.py 
```


### Q-table

The distance is computed as the sum of the absolute L1 (manhattan) distances from each state action pair of experiments. Divided by the geometric meanfrom action value functions:

```
d, Qi, Qj = 0
for s in states:
	for a in actions
		d += |qi(s, a) - qj(s, a)|
		Qi += qi(s, a)
		Qj += qj(s, a)

return d / sqrt(Qi * Qj)
```


| 1 | 2  | 3  | 4  | 5  | 6  |
|:-:|:--:|:--:|:--:|:--:|:--:|
|0. |0.27|0.26|0.17|0.31|0.18|
|0. |0.  |0.26|0.21|0.26|0.21|
|0. |0.  |0.  |0.13|0.09|0.14|
|0. |0.  |0.  |0.  |0.14|0.13|
|0. |0.  |0.  |0.  |0.  |0.23|
|0. |0.  |0.  |0.  |0.  |0.  |

### Policy distance

From the Q-tables we generate deterministic policies by making a hard max from each state. We perform the jaccard distance as 1 - jaccard simetry as a measurement of policy a and policy b disagreement. Jaccard similarity can be measured by ; JS = |S and T| / |S or T| where S and T are sets.


| 1  | 2  | 3  | 4  | 5  | 6  |
|:--:|:--:|:--:|:--:|:--:|:--:|
|0.  |0.  |0.  |0.  |0.  |0.  |
|0.  |0.  |0.  |0.  |0.  |0.  |
|0.  |0.  |0.  |0.  |0.  |0.  |
|0.  |0.  |0.  |0.  |0.  |0.  |
|0.  |0.  |0.  |0.  |0.  |0.  |
|0.  |0.  |0.  |0.  |0.  |0.  |

## Speeds, Counts and Rewards

The following figures are generated by running:

```
> python analysis/rewards.py cycles -t train
```

### Speeds and Counts

![Left axis shows average speeds, right axis shows the average counts over the 6 experiments during 5000 cycles.](60x30_train_velsvehs_cycles.png)

###  Rewards

![Average rewards per cycles](60x30_train_rewards_cycles.png)


###  Optimal 

![Ratio of the time the agent selects the optimal action](60x30_train_optimal_cycles.png)

## Scatter

The scatter plot is generated by running:

```
> python analysis/scatter.py
```

The figure shows the observation spaces perceived by the agent.
	 
- phase#0 blue dots relate to the pairs (s0, c0).
- phase#1 red dots relate to the pairs (s1, c1).
- target velocity on purple.
- gray lines are state separators;
- all blue points are on state (0, 0, 0, 0)
- left cluster: states (2, 0, 0, 0), (2, 1, 0, 0)
- every point on left cluster has penalty 0.

![This scatter plot shows two clouds of points on the speed vs counts space those related from phase 0 and 1](6030.png)

## Discussions

- Distance Matrix shows there's a lot of variation on Q-tables.
- Jaccard Distance Matrix is zero i.e policies are consistent.
- Both speed and count cannot be mutually high.
- Avg cycle return gets minimum penalty more frequently with more cycles.
- Ratio of optimal action approaches 1 as cycles grow.
- Scatter plot shows three clusters: Null, high count and high speed.

Experiments indicate that are policies are consistent and approach the
optimal.
